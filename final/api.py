from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import logging
import time
import os
import base64
import io
import tempfile
from contextlib import asynccontextmanager
from pydantic import BaseModel
from PIL import Image
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import cv2
import numpy as np
from insightface.app import FaceAnalysis
import pickle
import numpy.linalg as LA
from typing import List, Optional

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

VLM_MODEL_PATH = os.getenv("VLM_MODEL_PATH", "Qwen/Qwen-VL-Chat-Int4")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

FACE_MODEL_NAME = 'buffalo_l'
DB_EMBEDDINGS_FILE = 'face_embeddings.npy'
LABELS_FILE = 'labels.pkl'
EMBEDDING_SIZE = 512
MAX_FACES_FOR_ADD = 1
MAX_FACES_FOR_CHECK = 8

class VLMGenerateRequest(BaseModel):
    image_base64: str
    prompt: str = None

class VLMGenerateResponse(BaseModel):
    description: str
    processing_time: float
    model_used: str

class FaceRequest(BaseModel):
    image_base64: str

class AddFaceRequest(FaceRequest):
    comment: str

class FaceMatchResult(BaseModel):
    bbox: List[float]
    found: bool
    name: Optional[str] = None
    confidence: Optional[float] = None
    message: str

class CheckFaceResponse(BaseModel):
    faces: List[FaceMatchResult]
    message: str

class HealthResponse(BaseModel):
    status: str
    vlm_loaded: bool
    face_recognition_loaded: bool
    device: str
    face_db_size: int

class FaceRecognitionSystem:
    def __init__(self):
        self.face_app = FaceAnalysis(name=FACE_MODEL_NAME)
        self.face_app.prepare(ctx_id=0, det_size=(640, 640))
        self.load_database()
    
    def load_database(self):
        if os.path.exists(DB_EMBEDDINGS_FILE) and os.path.exists(LABELS_FILE):
            self.face_embeddings = np.load(DB_EMBEDDINGS_FILE)
            with open(LABELS_FILE, 'rb') as f:
                self.face_labels = pickle.load(f)
            logger.info(f"Ğ‘Ğ°Ğ·Ğ° Ğ»Ğ¸Ñ† Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°: {len(self.face_labels)} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹")
        else:
            self.face_embeddings = np.empty((0, EMBEDDING_SIZE), dtype=np.float32)
            self.face_labels = []
            logger.info("Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ»Ğ¸Ñ†")
    
    def save_database(self):
        np.save(DB_EMBEDDINGS_FILE, self.face_embeddings)
        with open(LABELS_FILE, 'wb') as f:
            pickle.dump(self.face_labels, f)
        logger.info("Ğ‘Ğ°Ğ·Ğ° Ğ»Ğ¸Ñ† ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ°")
    
    def base64_to_image(self, base64_str):
        try:
            if 'base64,' in base64_str:
                base64_str = base64_str.split('base64,')[1]
            image_data = base64.b64decode(base64_str)
            image = Image.open(io.BytesIO(image_data))
            return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ base64: {str(e)}")
            return None
    
    def process_image_for_add(self, image):
        if image is None:
            return None, "ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ"
        try:
            img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            faces = self.face_app.get(img_rgb)
            if len(faces) == 0:
                return None, "Ğ›Ğ¸Ñ†Ğ° Ğ½Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹"
            if len(faces) > MAX_FACES_FOR_ADD:
                return None, "ĞĞ° Ñ„Ğ¾Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°. ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ñ„Ğ¾Ñ‚Ğ¾ Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ»Ğ¸Ñ†Ğ¾Ğ¼."
            
            main_face = faces[0]
            return main_face.normed_embedding, None
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: {str(e)}")
            return None, "ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ"
    
    def process_image_for_check(self, image):
        if image is None:
            return [], "ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ"
        try:
            img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            faces = self.face_app.get(img_rgb)
            if len(faces) == 0:
                return [], "Ğ›Ğ¸Ñ†Ğ° Ğ½Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹"
            if len(faces) > MAX_FACES_FOR_CHECK:
                faces = faces[:MAX_FACES_FOR_CHECK]
            
            results = []
            for face in faces:
                bbox = face.bbox.tolist()
                embedding = face.normed_embedding
                results.append((bbox, embedding))
            
            return results, None
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: {str(e)}")
            return [], "ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ"
    
    def add_face(self, base64_str, comment):
        if not comment or comment.strip() == "":
            return "ĞšĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹ Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿ÑƒÑÑ‚Ñ‹Ğ¼"
        
        image = self.base64_to_image(base64_str)
        if image is None:
            return "ĞĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· base64"
        
        embedding, error = self.process_image_for_add(image)
        if error:
            return error
        
        if len(self.face_labels) > 0:
            dists = LA.norm(self.face_embeddings - np.array(embedding).reshape(1, -1), axis=1)
            min_dist = np.min(dists)
            if min_dist < 0.5:
                duplicate_index = np.argmin(dists)
                duplicate_name = self.face_labels[duplicate_index]
                return f"Ğ›Ğ¸Ñ†Ğ¾ ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ² Ğ±Ğ°Ğ·Ğµ ĞºĞ°Ğº '{duplicate_name}'"
        
        try:
            self.face_embeddings = np.vstack([self.face_embeddings, np.array(embedding).reshape(1, -1)])
            self.face_labels.append(comment)
            self.save_database()
            return f"Ğ›Ğ¸Ñ†Ğ¾ '{comment}' ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ² Ğ±Ğ°Ğ·Ñƒ"
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ†Ğ°: {str(e)}")
            return "ĞĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ»Ğ¸Ñ†Ğ¾ Ğ² Ğ±Ğ°Ğ·Ñƒ"
    
    def check_face(self, base64_str):
        image = self.base64_to_image(base64_str)
        if image is None:
            return CheckFaceResponse(
                faces=[],
                message="ĞĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· base64"
            )
        
        faces, error = self.process_image_for_check(image)
        if error:
            return CheckFaceResponse(
                faces=[],
                message=error
            )
        
        if len(self.face_labels) == 0:
            return CheckFaceResponse(
                faces=[],
                message="Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»Ğ¸Ñ† Ğ¿ÑƒÑÑ‚Ğ°"
            )
        
        results = []
        for bbox, embedding in faces:
            dists = LA.norm(self.face_embeddings - np.array(embedding).reshape(1, -1), axis=1)
            min_index = np.argmin(dists)
            min_dist = dists[min_index]
            
            if min_dist < 0.6:
                confidence = max(0, 100 - min_dist * 100)
                results.append(FaceMatchResult(
                    bbox=bbox,
                    found=True,
                    name=self.face_labels[min_index],
                    confidence=confidence,
                    message=f"ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾: {self.face_labels[min_index]} ({confidence:.1f}%)"
                ))
            else:
                results.append(FaceMatchResult(
                    bbox=bbox,
                    found=False,
                    message="ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ² Ğ±Ğ°Ğ·Ğµ"
                ))
        
        return CheckFaceResponse(
            faces=results,
            message=f"ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ {len(results)} Ğ»Ğ¸Ñ†"
        )

@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº API...")
    
    try:
        app.state.tokenizer = AutoTokenizer.from_pretrained(
            VLM_MODEL_PATH,
            trust_remote_code=True
        )
        app.state.model = AutoModelForCausalLM.from_pretrained(
            VLM_MODEL_PATH,
            device_map=DEVICE,
            trust_remote_code=True
        ).eval()
        logger.info(f"âœ… VLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ½Ğ° {DEVICE}")
        app.state.vlm_loaded = True
    except Exception as e:
        logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ VLM Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {str(e)}")
        app.state.vlm_loaded = False
    
    try:
        app.state.face_system = FaceRecognitionSystem()
        logger.info("âœ… Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¸Ñ† Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ°")
        app.state.face_recognition_loaded = True
    except Exception as e:
        logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¸Ñ†: {str(e)}")
        app.state.face_recognition_loaded = False
        app.state.face_system = None
    
    yield
    
    logger.info("ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° API...")

app = FastAPI(
    title="Multimodal API",
    description="API Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¸Ñ†",
    version="1.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

DEFAULT_PROMPT = (
    "Describe what you see in the photo. Speak like with a person from 8 to 14 years old. "
    "Speak only English. In math tasks, 'x' is a mathematical variable, not multiplication. "
    "Example: for 6x + 5 = 23, it's 6x = 23-5 â†’ 6x = 18 â†’ x = 3"
)

@app.get("/health", response_model=HealthResponse)
async def health_check():
    face_db_size = len(app.state.face_system.face_labels) if app.state.face_system else 0
    return HealthResponse(
        status="healthy",
        vlm_loaded=app.state.vlm_loaded,
        face_recognition_loaded=app.state.face_recognition_loaded,
        device=DEVICE,
        face_db_size=face_db_size
    )

@app.post("/vlm/describe", response_model=VLMGenerateResponse)
async def describe_image(request: VLMGenerateRequest):
    start_time = time.time()
    
    if not hasattr(app.state, "model") or not app.state.vlm_loaded:
        raise HTTPException(status_code=503, detail="VLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°")
    
    tmp_path = None
    try:
        base64_str = request.image_base64
        prompt = request.prompt if request.prompt else DEFAULT_PROMPT
        
        if ";base64," in base64_str:
            base64_str = base64_str.split(";base64,")[1]
        
        image_data = base64.b64decode(base64_str)
        img = Image.open(io.BytesIO(image_data))
        
        if img.mode != "RGB":
            img = img.convert("RGB")
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp:
            img.save(tmp, format="JPEG")
            tmp_path = tmp.name
        
        query = app.state.tokenizer.from_list_format([
            {'image': tmp_path},
            {'text': prompt},
        ])
        
        with torch.no_grad():
            response, _ = app.state.model.chat(
                tokenizer=app.state.tokenizer,
                query=query,
                history=None
            )
        
        if tmp_path and os.path.exists(tmp_path):
            os.unlink(tmp_path)
        
        processing_time = time.time() - start_time
        logger.info(f"âœ… ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ·Ğ° {processing_time:.2f}Ñ")
        
        return VLMGenerateResponse(
            description=response,
            processing_time=processing_time,
            model_used=VLM_MODEL_PATH
        )
    
    except Exception as e:
        if tmp_path and os.path.exists(tmp_path):
            os.unlink(tmp_path)
        logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: {str(e)}")

@app.post("/face/add")
async def add_face(request: AddFaceRequest):
    if not app.state.face_system or not app.state.face_recognition_loaded:
        raise HTTPException(status_code=503, detail="Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¸Ñ† Ğ½Ğµ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ°")
    
    try:
        result = app.state.face_system.add_face(request.image_base64, request.comment)
        return {"result": result}
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ»Ğ¸Ñ†Ğ°: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° ÑĞµÑ€Ğ²ĞµÑ€Ğ°: {str(e)}")

@app.post("/face/check", response_model=CheckFaceResponse)
async def check_face(request: FaceRequest):
    if not app.state.face_system or not app.state.face_recognition_loaded:
        raise HTTPException(status_code=503, detail="Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¸Ñ† Ğ½Ğµ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ°")
    
    try:
        return app.state.face_system.check_face(request.image_base64)
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ğ»Ğ¸Ñ†Ğ°: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° ÑĞµÑ€Ğ²ĞµÑ€Ğ°: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)