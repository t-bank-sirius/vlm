from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import logging
import time
import os
import base64
import io
import tempfile
from contextlib import asynccontextmanager
from pydantic import BaseModel
from PIL import Image
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import cv2
import numpy as np
from insightface.app import FaceAnalysis
import pickle
import numpy.linalg as LA

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

VLM_MODEL_PATH = os.getenv("VLM_MODEL_PATH", "/app/model")
VLM_MODEL_PATH = "/app/model"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


INSIGHTFACE_ROOT = "/app/.insightface"
FACE_MODEL_NAME = 'buffalo_l'
DB_EMBEDDINGS_FILE = 'face_embeddings.npy'
LABELS_FILE = 'labels.pkl'
EMBEDDING_SIZE = 512
MAX_FACES_FOR_ADD = 1
MAX_FACES_FOR_CHECK = 8

class FaceAddRequest(BaseModel):
    image_base64: str
    comment: str

class AnalyzeRequest(BaseModel):
    image_base64: str
    prompt: str

class FaceAddResponse(BaseModel):
    result: str

class AnalyzeResponse(BaseModel):
    result: str
    processing_time: float

class FaceRecognitionSystem:
    def __init__(self):
        self.face_app = FaceAnalysis(name=FACE_MODEL_NAME, root=INSIGHTFACE_ROOT)
        self.face_app.prepare(ctx_id=0, det_size=(640, 640))
        self.load_database()
    
    def load_database(self):
        if os.path.exists(DB_EMBEDDINGS_FILE) and os.path.exists(LABELS_FILE):
            self.face_embeddings = np.load(DB_EMBEDDINGS_FILE)
            with open(LABELS_FILE, 'rb') as f:
                self.face_labels = pickle.load(f)
            logger.info(f"–ë–∞–∑–∞ –ª–∏—Ü –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(self.face_labels)} –∑–∞–ø–∏—Å–µ–π")
        else:
            self.face_embeddings = np.empty((0, EMBEDDING_SIZE), dtype=np.float32)
            self.face_labels = []
            logger.info("–°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –±–∞–∑–∞ –ª–∏—Ü")
    
    def save_database(self):
        np.save(DB_EMBEDDINGS_FILE, self.face_embeddings)
        with open(LABELS_FILE, 'wb') as f:
            pickle.dump(self.face_labels, f)
        logger.info("–ë–∞–∑–∞ –ª–∏—Ü —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    
    def base64_to_image(self, base64_str):
        try:
            if 'base64,' in base64_str:
                base64_str = base64_str.split('base64,')[1]
            image_data = base64.b64decode(base64_str)
            image = Image.open(io.BytesIO(image_data))
            return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è base64: {str(e)}")
            return None
    
    def detect_main_face(self, image):
        if image is None:
            return None, "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
        try:
            img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            faces = self.face_app.get(img_rgb)
            
            if len(faces) == 0:
                return None, "–õ–∏—Ü–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã"
            
            main_face = sorted(faces, key=lambda x: (x.bbox[2]-x.bbox[0])*(x.bbox[3]-x.bbox[1]), reverse=True)[0]
            return main_face.normed_embedding, None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}")
            return None, "–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"
    
    def detect_all_faces(self, image):
        if image is None:
            return [], "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
        try:
            img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            faces = self.face_app.get(img_rgb)
            
            if len(faces) == 0:
                return [], "–õ–∏—Ü–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã"
            
            if len(faces) > MAX_FACES_FOR_CHECK:
                faces = faces[:MAX_FACES_FOR_CHECK]
            
            embeddings = [face.normed_embedding for face in faces]
            return embeddings, None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}")
            return [], "–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"
    
    def add_face(self, base64_str, comment):
        if not comment or comment.strip() == "":
            return "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º"
        
        image = self.base64_to_image(base64_str)
        if image is None:
            return "–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ base64"
        
        embedding, error = self.detect_main_face(image)
        if error:
            return error
        
        if len(self.face_labels) > 0:
            if len(self.face_labels) != len(self.face_embeddings):
                logger.error(
                    f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –±–∞–∑—ã: –º–µ—Ç–∫–∏={len(self.face_labels)}, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏={len(self.face_embeddings)}"
                )
                min_length = min(len(self.face_labels), len(self.face_embeddings))
                self.face_labels = self.face_labels[:min_length]
                self.face_embeddings = self.face_embeddings[:min_length]
                self.save_database()
                logger.warning(f"–ë–∞–∑–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –¥–æ {min_length} –∑–∞–ø–∏—Å–µ–π")
            
            dists = LA.norm(self.face_embeddings - np.array(embedding).reshape(1, -1), axis=1)
            min_dist = np.min(dists)
            if min_dist < 0.5:
                duplicate_index = np.argmin(dists)
                
                if duplicate_index < len(self.face_labels):
                    duplicate_name = self.face_labels[duplicate_index]
                    return f"–ß–µ–ª–æ–≤–µ–∫ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –±–∞–∑–µ –∫–∞–∫ '{duplicate_name}'"
                else:
                    logger.error(
                        f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞: {duplicate_index} (—Ä–∞–∑–º–µ—Ä –º–µ—Ç–æ–∫: {len(self.face_labels)})"
                    )
                    return "–û—à–∏–±–∫–∞: –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑–µ –ª–∏—Ü"
        
        try:
            self.face_embeddings = np.vstack([self.face_embeddings, np.array(embedding).reshape(1, -1)])
            self.face_labels.append(comment)
            self.save_database()
            return f"Ok! I'll remember that it's {comment}"
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ª–∏—Ü–∞: {str(e)}")
            return "–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–∏—Ü–æ –≤ –±–∞–∑—É"
    
    def recognize_faces(self, base64_str):
        image = self.base64_to_image(base64_str)
        if image is None:
            return [], "–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ base64"
        
        embeddings, error = self.detect_all_faces(image)
        if error:
            return [], error
        
        if len(self.face_labels) == 0:
            return [], "–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ª–∏—Ü –ø—É—Å—Ç–∞"
        
        results = []
        for embedding in embeddings:
            dists = LA.norm(self.face_embeddings - np.array(embedding).reshape(1, -1), axis=1)
            min_index = np.argmin(dists)
            min_dist = dists[min_index]
            
            if min_dist < 0.6:
                confidence = max(0, 100 - min_dist * 100)
                results.append({
                    "name": self.face_labels[min_index],
                    "confidence": confidence
                })
        
        return results, None

@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ API...")
    
    try:
        app.state.tokenizer = AutoTokenizer.from_pretrained(
            VLM_MODEL_PATH,
            trust_remote_code=True
        )
        app.state.model = AutoModelForCausalLM.from_pretrained(
            VLM_MODEL_PATH,
            device_map=DEVICE,
            trust_remote_code=True
        ).eval()
        logger.info(f"‚úÖ VLM –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {DEVICE}")
        app.state.vlm_loaded = True
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ VLM –º–æ–¥–µ–ª–∏: {str(e)}")
        app.state.vlm_loaded = False
    
    try:
        app.state.face_system = FaceRecognitionSystem()
        logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        app.state.face_recognition_loaded = True
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü: {str(e)}")
        app.state.face_recognition_loaded = False
        app.state.face_system = None
    
    yield
    
    logger.info("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ API...")

app = FastAPI(
    title="Multimodal Analysis API",
    description="API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è –ª–∏—Ü –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π",
    version="1.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

DEFAULT_PROMPT = (
    "Describe what you see in this picture in details. "
    "Speak only English. In math tasks, 'x' is a mathematical variable, not multiplication. Example: 6x + 5 = 23. You dont need to solve them. "
    "If you see pictures with the content of weapons, drugs, violence, murders, etc., write that this picture contains prohibited content that I cannot describe"
)

async def generate_description(image_base64: str, prompt: str) -> str:
    start_time = time.time()
    
    if not hasattr(app.state, "model") or not app.state.vlm_loaded:
        return "VLM –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
    
    tmp_path = None
    try:
        if ";base64," in image_base64:
            image_base64 = image_base64.split(";base64,")[1]
        
        image_data = base64.b64decode(image_base64)
        img = Image.open(io.BytesIO(image_data))
        
        if img.mode != "RGB":
            img = img.convert("RGB")
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp:
            img.save(tmp, format="JPEG")
            tmp_path = tmp.name
        
        final_prompt = DEFAULT_PROMPT
        if prompt and prompt.strip():
            final_prompt = DEFAULT_PROMPT + " " + prompt.strip()
        
        query = app.state.tokenizer.from_list_format([
            {'image': tmp_path},
            {'text': final_prompt},
        ])
        
        with torch.no_grad():
            response, _ = app.state.model.chat(
                tokenizer=app.state.tokenizer,
                query=query,
                history=None
            )
        
        if tmp_path and os.path.exists(tmp_path):
            os.unlink(tmp_path)
        
        logger.info(f"‚úÖ –û–ø–∏—Å–∞–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∑–∞ {time.time() - start_time:.2f}—Å")
        return response
    
    except Exception as e:
        if tmp_path and os.path.exists(tmp_path):
            os.unlink(tmp_path)
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è: {str(e)}")
        return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è: {str(e)}"

@app.post("/add", response_model=FaceAddResponse)
async def add_face_endpoint(request: FaceAddRequest):
    start_time = time.time()
    
    if not app.state.face_recognition_loaded or not app.state.face_system:
        return FaceAddResponse(result="–°–∏—Å—Ç–µ–º–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    result = app.state.face_system.add_face(request.image_base64, request.comment)
    
    return FaceAddResponse(
        result=result,
    )

@app.post("/analyze", response_model=AnalyzeResponse)
async def analyze_image(request: AnalyzeRequest):
    start_time = time.time()
    
    face_detection_result = "–õ–∏—Ü –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ"
    recognized_faces = []
    
    if app.state.face_recognition_loaded and app.state.face_system:
        image = app.state.face_system.base64_to_image(request.image_base64)
        if image is not None:
            embeddings, error = app.state.face_system.detect_all_faces(image)
            
            if embeddings:
                face_detection_result = f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ª–∏—Ü: {len(embeddings)}"
                
                recognized, _ = app.state.face_system.recognize_faces(request.image_base64)
                if recognized:
                    recognized_faces = [f"{face['name']}" for face in recognized]
    
    if "–Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ" in face_detection_result.lower():
        description = await generate_description(request.image_base64, request.prompt)
        return AnalyzeResponse(
            result=description,
            processing_time=time.time() - start_time
        )
    
    if recognized_faces:
        identity_msg = "I remember this person! This is " + ", ".join(recognized_faces) + ". "
        
        description = await generate_description(request.image_base64, request.prompt)
        
        return AnalyzeResponse(
            result=identity_msg + description,
            processing_time=time.time() - start_time
        )
    else:
        description = await generate_description(request.image_base64, request.prompt)
        return AnalyzeResponse(
            result=description,
            processing_time=time.time() - start_time
        )
    
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
